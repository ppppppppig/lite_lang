### 1.背景

一般对MLP使用算子融合的方法优化，比如qwen2的MLP层代码如下：
```
def forward(self, x):
    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
    return down_proj
```
对该算子流程分析，可以看出有以下几个优化点：

1.self.gate_proj和self.up_proj均是对张量x做矩阵乘法，可以将两者合并为一个张量，计算完成后再分割，这样可以少计算一次矩阵乘法。（本质上是减少了一次加载张量x的开销）

2.可以将激活函数和逐元素乘法结合起来，减少一次中间结果的存取，并且减少kernel launch，减少内存压力

个人感觉最好不要将整个MLP流程封装成一个大的MLP算子，因为MLP层包含比较多的MM算子，这些算子pytorch已经开发的比较成熟了，我们重新使用triton封装不仅开发难度大，而且很难达到pytorch的性能。


### 2.概念

#### 2.1 算子融合

算子融合的优化点主要是以下几个方面：

1.多个算子融合成一个算子显著减少kernel launch开销，于是降低了CPU负载，减少了kernel launch对整体性能可能得影响

2.算子融合可以减少中间变量的存储到显存，减少访问HBM的次数，减少了显存访问

#### 2.2 激活函数

激活函数的主要作用是保证训练时梯度更新的稳定，避免梯度震荡，梯度过大过小等问题

而silu的梯度更新函数为：

$$
    f(x) = x \cdot \sigma(x) = x \cdot \frac{1}{1 + e^{-x}}
$$

它的优势在于，函数整体是连续平滑的，解决了梯度消失的问题。

#### 2.3 torch.jit

torch.jit通过将pytorch编译成中间表示，主要有以下几点优化：

1.减少了python动态编译耗时

2.构建成静态图后进行图优化，使用算子融合等手段，从而减少访问显存次数（自动算子融合感觉主要是定义了几套规则，出现了类似的情况则进行融合）

3.对比运行前编译，能够针对特定硬件进行优化，对比运行时动态编译，可以减少内存动态分配带来的开销等
